import os
import json
import requests
import cloudscraper
import yaml
import gzip
import logging
from datetime import datetime, timedelta
from pathlib import Path
from urllib.parse import urlsplit, urlunsplit
from bs4 import BeautifulSoup

# ========== åŸºç¡€è®¾ç½® ==========
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# ========== é…ç½® ==========
def load_config(path="config.yaml"):
    with open(path, encoding="utf-8") as f:
        return yaml.safe_load(f) or {}

def get_feishu_webhook(config):
    return os.getenv("FEISHU_WEBHOOK") or config.get("feishu", {}).get("webhook_url")

# ========== URL è§„èŒƒåŒ–ï¼ˆå…³é”®ä¿®å¤ç‚¹ï¼‰ ==========
def normalize_url(url: str) -> str:
    """
    ä¿ç•™ scheme + domain + path
    å»æ‰ ?query å’Œ #fragment
    """
    try:
        parts = urlsplit(url)
        return urlunsplit((parts.scheme, parts.netloc, parts.path, "", ""))
    except Exception:
        return url

# ========== URL è¿‡æ»¤ï¼ˆé™å™ªå…³é”®ï¼‰ ==========
EXCLUDE_KEYWORDS = [
    "/tag/",
    "/tags/",
    "/category/",
    "/categories/",
    "/about",
    "/privacy",
    "/terms",
    "/contact",
    "/faq",
    "/policy",
    "/search",
    "/sitemap",
    "/wp-",
]

def is_valid_game_url(url: str) -> bool:
    u = url.lower()

    # 1. é»‘åå•å…³é”®è¯
    for k in EXCLUDE_KEYWORDS:
        if k in u:
            return False

    # 2. å¤ªçŸ­çš„è·¯å¾„ï¼ˆé€šå¸¸ä¸æ˜¯è¯¦æƒ…é¡µï¼‰
    try:
        path = urlsplit(u).path
        if path.count("/") < 2:
            return False
    except Exception:
        return False

    return True


# ========== Sitemap å¤„ç† ==========
def process_sitemap(url):
    try:
        scraper = cloudscraper.create_scraper()
        resp = scraper.get(url, timeout=20)
        resp.raise_for_status()

        content = resp.content
        if content[:2] == b"\x1f\x8b":
            content = gzip.decompress(content)

        if b"<urlset" in content or b"<sitemapindex" in content:
            return parse_xml(content)
        else:
            return parse_txt(content.decode("utf-8", errors="ignore"))

    except Exception as e:
        logging.error(f"Sitemap error {url}: {e}")
        return []

def parse_xml(content):
    soup = BeautifulSoup(content, "xml")
    urls = []
    for loc in soup.find_all("loc"):
        u = loc.get_text().strip()
        if u:
            nu = normalize_url(u)
            if is_valid_game_url(nu):
            urls.append(nu)
    return urls

def parse_txt(text):
    urls = []
    for line in text.splitlines():
        line = line.strip()
        if line.startswith("http"):
            nu = normalize_url(line)
            if is_valid_game_url(nu):
                urls.append(nu)
    return urls

# ========== æ•°æ®å­˜å‚¨ ==========
def save_latest(site, urls):
    Path("latest").mkdir(exist_ok=True)
    with open(f"latest/{site}.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(urls))

def load_latest(site):
    path = Path(f"latest/{site}.txt")
    if not path.exists():
        return set()
    return set(x.strip() for x in path.read_text(encoding="utf-8").splitlines())

def save_diff(site, urls):
    today = datetime.now().strftime("%Y%m%d")
    folder = Path("diff") / today
    folder.mkdir(parents=True, exist_ok=True)

    with open(folder / f"{site}.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(urls))

# ========== é£ä¹¦é€šçŸ¥ ==========
def send_feishu(site, urls, config):
    if not urls:
        return

    webhook = get_feishu_webhook(config)
    if not webhook:
        return

    content = "\n".join(f"â€¢ {u}" for u in urls[:10])
    payload = {
        "msg_type": "interactive",
        "card": {
            "header": {
                "title": {"tag": "plain_text", "content": f"ğŸ® {site} æ¸¸æˆä¸Šæ–°"},
                "template": "green"
            },
            "elements": [
                {
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": f"**ä»Šæ—¥æ–°å¢ {len(urls)} æ¡**\n\n{content}"
                    }
                }
            ]
        }
    }

    requests.post(webhook, json=payload, timeout=10)

# ========== æ¸…ç†å†å² ==========
def cleanup(config):
    days = config.get("storage", {}).get("retention_days", 7)
    cutoff = datetime.now() - timedelta(days=days)

    for d in Path("diff").glob("*"):
        try:
            date = datetime.strptime(d.name, "%Y%m%d")
            if date < cutoff:
                for f in d.glob("*"):
                    f.unlink()
                d.rmdir()
        except:
            pass

# ========== ä¸»æµç¨‹ ==========
def main():
    config = load_config()

    for site in config.get("sites", []):
        if not site.get("active"):
            continue

        name = site["name"]
        logging.info(f"Processing {name}")

        all_urls = []
        for sm in site.get("sitemap_urls", []):
            all_urls.extend(process_sitemap(sm))

        # å»é‡ï¼ˆé¡ºåºç¨³å®šï¼‰
        current = list(dict.fromkeys(all_urls))
        last = load_latest(name)

        new_urls = [u for u in current if u not in last]

        save_latest(name, current)

        if new_urls:
            save_diff(name, new_urls)
            send_feishu(name, new_urls, config)

    cleanup(config)

if __name__ == "__main__":
    main()
